import streamlit as st
import os 

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import RetrievalQA
from langchain import hub
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore


load_dotenv() # í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸° 

# í˜ì´ì§€ ì»´í”½ 
st.set_page_config(page_title="ì†Œë“ì„¸ ì±—ë´‡", page_icon="ğŸ‘»")

# íƒ€ì´í‹€ ì„¤ì •
st.title("ğŸ‘»ì†Œë“ì„¸ ì±—ë´‡ğŸ‘»") # h1 íƒœê·¸ë¡œ ìƒì„±ë¨ 
st.caption("ì†Œë“ì„¸ì— ê´€ë ¨ëœ ëª¨ë“  ê²ƒì„ ë‹µí•´ë“œë ¤ìš”!")


# ì…ë ¥ëœ ì±„íŒ…ë“¤ì„ Session State ì— ì €ì¥ 
# message_list ì— ì±„íŒ…í•œ ê°’ë“¤ì„ ê³„ì† ë„£ì–´ì¤Œ 
if 'message_list' not in st.session_state:
    st.session_state.message_list = []

# UI ë³€ê²½ì‚¬í•­ ì—…ë°ì´íŠ¸ 
for message in st.session_state.message_list:
    with st.chat_message(message["role"]):
        st.write(message["content"])


def get_ai_message(user_memsage):
    embedding = OpenAIEmbeddings(model='text-embedding-3-large') 
    index_name = 'tax-markdown-index'
    database = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embedding)

    llm = ChatOpenAI(model='gpt-4o')
    prompt = hub.pull("rlm/rag-prompt")
    retriever = database.as_retriever(search_kwargs={'k': 7})

    # QA chain ë§Œë“¤ê¸° 
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm, 
        retriever=retriever, 
        chain_type_kwargs={"prompt": prompt}
    )

    dictionary = ["ì‚¬ëŒì„ ë‚˜íƒ€ë‚´ëŠ” í‘œí˜„ -> ê±°ì£¼ì"]
    prompt = ChatPromptTemplate.from_template(f"""
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³´ê³ , ìš°ë¦¬ì˜ ì‚¬ì „ì„ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•´ì£¼ì„¸ìš”.
        ë§Œì•½ ë³€ê²½í•  í•„ìš”ê°€ ì—†ë‹¤ê³  íŒë‹¨ëœë‹¤ë©´, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤. 
        ê·¸ëŸ° ê²½ìš°ì—ëŠ” {{question}}
        ì‚¬ì „: {dictionary}
        ì§ˆë¬¸: {{question}}
    """)

    # query -> ì§ì¥ì¸ -> ê±°ì£¼ì chain ì¶”ê°€ 
    dictionary_chain = prompt | llm | StrOutputParser()
    tax_chain = {"query": dictionary_chain} | qa_chain
    ai_message = tax_chain.invoke({"question": user_memsage})
    return ai_message["result"]


# name ì€ user, assistant, ai, human ë“± 
# ì…ë ¥ë  ë•Œë§ˆë‹¤, ì „ì²´ ì½”ë“œê°€ ë‹¤ì‹œ ì‹¹ ëˆë‹¤ 
if user_question := st.chat_input(placeholder="ì†Œë“ì„¸ì— ê´€ë ¨ëœ ê¶ê¸ˆí•œ ë‚´ìš©ë“¤ì„ ë§ì”€í•´ì£¼ì„¸ìš”."):
    with st.chat_message("user"):
        st.write(user_question)
    st.session_state.message_list.append({"role" : "user", "content" : user_question})

    with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤"):
        ai_message = get_ai_message(user_question)

        with st.chat_message("ai"):
            st.write(ai_message)
        st.session_state.message_list.append({"role" : "ai", "content" : ai_message})
